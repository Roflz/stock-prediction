{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMdpyzOpyIQ+gONJBrGtbgr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roflz/stock-prediction/blob/main/predict_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Installs/upgrades packages needed for running stock prediction program using Google Colab\n",
        "# After installing/upgrading, Go to Runtime -> Restart runtime to apply changes\n",
        "\n",
        "!pip install --upgrade pandas-datareader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM_mXb8M_vm9",
        "outputId": "f715425a-5461-430d-e258-4145d79439d4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas-datareader in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from pandas-datareader) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pandas-datareader) (2.23.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pandas-datareader) (4.9.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->pandas-datareader) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->pandas-datareader) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->pandas-datareader) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23->pandas-datareader) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBAlO5uNlGAi",
        "outputId": "7a364064-6e9c-4c35-cd47-f036d176a911"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  High         Low        Open       Close       Volume  \\\n",
            "Date                                                                      \n",
            "2017-08-07   43.092499   42.000000   42.097500   43.087502   71741200.0   \n",
            "2017-08-08   43.639999   42.177502   43.472500   42.575001   75533600.0   \n",
            "2017-08-09   43.052502   41.917500   42.107498   43.027500   53078400.0   \n",
            "2017-08-10   43.165001   41.082500   43.040001   41.185001  132808800.0   \n",
            "2017-08-11   39.750000   38.227501   39.285000   38.990002  149719600.0   \n",
            "...                ...         ...         ...         ...          ...   \n",
            "2022-07-28  181.399994  174.399994  179.750000  179.839996   47464600.0   \n",
            "2022-07-29  182.440002  176.919998  178.130005  181.630005   43520200.0   \n",
            "2022-08-01  188.460007  179.899994  181.820007  184.410004   47646900.0   \n",
            "2022-08-02  189.380005  180.919998  181.220001  185.259995   48952700.0   \n",
            "2022-08-03  189.679993  181.369995  181.839996  188.929993   41773400.0   \n",
            "\n",
            "             Adj Close  \n",
            "Date                    \n",
            "2017-08-07   42.609219  \n",
            "2017-08-08   42.102406  \n",
            "2017-08-09   42.549892  \n",
            "2017-08-10   40.727837  \n",
            "2017-08-11   38.557209  \n",
            "...                ...  \n",
            "2022-07-28  179.839996  \n",
            "2022-07-29  181.630005  \n",
            "2022-08-01  184.410004  \n",
            "2022-08-02  185.259995  \n",
            "2022-08-03  188.929993  \n",
            "\n",
            "[1257 rows x 6 columns]\n",
            "Epoch 1/50\n",
            "30/30 [==============================] - 15s 222ms/step - loss: 0.0109\n",
            "Epoch 2/50\n",
            "30/30 [==============================] - 9s 297ms/step - loss: 0.0031\n",
            "Epoch 3/50\n",
            "30/30 [==============================] - 20s 683ms/step - loss: 0.0030\n",
            "Epoch 4/50\n",
            "30/30 [==============================] - 11s 353ms/step - loss: 0.0028\n",
            "Epoch 5/50\n",
            "30/30 [==============================] - 8s 253ms/step - loss: 0.0032\n",
            "Epoch 6/50\n",
            "30/30 [==============================] - 10s 355ms/step - loss: 0.0033\n",
            "Epoch 7/50\n",
            "30/30 [==============================] - 7s 235ms/step - loss: 0.0023\n",
            "Epoch 8/50\n",
            "30/30 [==============================] - 8s 255ms/step - loss: 0.0021\n",
            "Epoch 9/50\n",
            "30/30 [==============================] - 8s 277ms/step - loss: 0.0020\n",
            "Epoch 10/50\n",
            "30/30 [==============================] - 8s 255ms/step - loss: 0.0028\n",
            "Epoch 11/50\n",
            "30/30 [==============================] - 9s 306ms/step - loss: 0.0021\n",
            "Epoch 12/50\n",
            "30/30 [==============================] - 8s 267ms/step - loss: 0.0023\n",
            "Epoch 13/50\n",
            "30/30 [==============================] - 6s 217ms/step - loss: 0.0019\n",
            "Epoch 14/50\n",
            "30/30 [==============================] - 9s 288ms/step - loss: 0.0018\n",
            "Epoch 15/50\n",
            "30/30 [==============================] - 8s 274ms/step - loss: 0.0019\n",
            "Epoch 16/50\n",
            "30/30 [==============================] - 8s 266ms/step - loss: 0.0016\n",
            "Epoch 17/50\n",
            "30/30 [==============================] - 7s 245ms/step - loss: 0.0016\n",
            "Epoch 18/50\n",
            "30/30 [==============================] - 7s 227ms/step - loss: 0.0015\n",
            "Epoch 19/50\n",
            "30/30 [==============================] - 12s 424ms/step - loss: 0.0014\n",
            "Epoch 20/50\n",
            "30/30 [==============================] - 8s 287ms/step - loss: 0.0015\n",
            "Epoch 21/50\n",
            "30/30 [==============================] - 9s 295ms/step - loss: 0.0014\n",
            "Epoch 22/50\n",
            "30/30 [==============================] - 26s 892ms/step - loss: 0.0015\n",
            "Epoch 23/50\n",
            "30/30 [==============================] - 16s 545ms/step - loss: 0.0019\n",
            "Epoch 24/50\n",
            "30/30 [==============================] - 13s 428ms/step - loss: 0.0014\n",
            "Epoch 25/50\n",
            "30/30 [==============================] - 14s 464ms/step - loss: 0.0014\n",
            "Epoch 26/50\n",
            "30/30 [==============================] - 8s 257ms/step - loss: 0.0013\n",
            "Epoch 27/50\n",
            "30/30 [==============================] - 7s 229ms/step - loss: 0.0014\n",
            "Epoch 28/50\n",
            "30/30 [==============================] - 7s 236ms/step - loss: 0.0016\n",
            "Epoch 29/50\n",
            "30/30 [==============================] - 8s 258ms/step - loss: 0.0014\n",
            "Epoch 30/50\n",
            "30/30 [==============================] - 8s 264ms/step - loss: 0.0012\n",
            "Epoch 31/50\n",
            "30/30 [==============================] - 9s 295ms/step - loss: 0.0012\n",
            "Epoch 32/50\n",
            "30/30 [==============================] - 12s 397ms/step - loss: 0.0012\n",
            "Epoch 33/50\n",
            "30/30 [==============================] - 8s 266ms/step - loss: 0.0011\n",
            "Epoch 34/50\n",
            "30/30 [==============================] - 12s 395ms/step - loss: 0.0012\n",
            "Epoch 35/50\n",
            "30/30 [==============================] - 8s 268ms/step - loss: 0.0014\n",
            "Epoch 36/50\n",
            "30/30 [==============================] - 7s 238ms/step - loss: 0.0012\n",
            "Epoch 37/50\n",
            "30/30 [==============================] - 7s 229ms/step - loss: 0.0012\n",
            "Epoch 38/50\n",
            "30/30 [==============================] - 6s 216ms/step - loss: 0.0011\n",
            "Epoch 39/50\n",
            "30/30 [==============================] - 8s 279ms/step - loss: 9.9154e-04\n",
            "Epoch 40/50\n",
            "30/30 [==============================] - 8s 255ms/step - loss: 0.0012\n",
            "Epoch 41/50\n",
            "30/30 [==============================] - 7s 227ms/step - loss: 0.0011\n",
            "Epoch 42/50\n",
            "30/30 [==============================] - 8s 269ms/step - loss: 0.0010\n",
            "Epoch 43/50\n",
            "30/30 [==============================] - 7s 230ms/step - loss: 0.0010\n",
            "Epoch 44/50\n",
            "30/30 [==============================] - 8s 256ms/step - loss: 0.0013\n",
            "Epoch 45/50\n",
            "30/30 [==============================] - 7s 228ms/step - loss: 0.0011\n",
            "Epoch 46/50\n",
            "30/30 [==============================] - 10s 341ms/step - loss: 0.0011\n",
            "Epoch 47/50\n",
            "30/30 [==============================] - 8s 280ms/step - loss: 0.0012\n",
            "Epoch 48/50\n",
            "30/30 [==============================] - 8s 283ms/step - loss: 0.0011\n",
            "Epoch 49/50\n",
            "30/30 [==============================] - 7s 234ms/step - loss: 0.0011\n",
            "Epoch 50/50\n",
            "30/30 [==============================] - 13s 446ms/step - loss: 0.0011\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas_datareader as pdr\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from google.colab import files\n",
        "\n",
        "# function to help create the datasets\n",
        "# for features (x), appends the last 50 prices\n",
        "# for labels (y), appends the next price\n",
        "def create_dataset(df):\n",
        "    x = []\n",
        "    y = []\n",
        "    for i in range(50, df.shape[0]):\n",
        "        x.append(df[i-50:i, 0])\n",
        "        y.append(df[i, 0])\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    return x,y\n",
        "\n",
        "# flattens a list of lists into 1 list\n",
        "def flatten(l):\n",
        "    return [item for sublist in l for item in sublist]\n",
        "\n",
        "# Request data via Yahoo public API\n",
        "# currently reads in 5 years of data as default\n",
        "df = pdr.get_data_yahoo('NVDA')\n",
        "print(df)\n",
        "\n",
        "# upload files and read csv (optional)\n",
        "# df = files.upload()\n",
        "# df = pd.read_csv(df)\n",
        "\n",
        "# get number of trading days\n",
        "df.shape\n",
        "\n",
        "# set dimensions of dataset\n",
        "df = df['Open'].values\n",
        "df = df.reshape(-1, 1)\n",
        "\n",
        "# split the data into training and testing sets\n",
        "# training set is taking 1st 20% of data points (Oldest 20% of data points)\n",
        "# test set is taking last 80% of data points (Most recent 80% of data points)\n",
        "dataset_train = np.array(df[:int(df.shape[0]*0.8)])\n",
        "dataset_test = np.array(df[int(df.shape[0]*0.8):])\n",
        "\n",
        "# scale data between 0 and 1\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "dataset_train = scaler.fit_transform(dataset_train)\n",
        "dataset_test = scaler.transform(dataset_test)\n",
        "\n",
        "# create datasets using function\n",
        "x_train, y_train = create_dataset(dataset_train)\n",
        "x_test, y_test = create_dataset(dataset_test)\n",
        "\n",
        "# initialize model as a sequential one with 96 units in the outputâ€™s dimensionality\n",
        "# use return_sequences=True to make the LSTM layer with three-dimensional input and input_shape to shape our dataset\n",
        "# Making the dropout fraction 0.2 drops 20% of the layers\n",
        "# Finally add a dense layer with a value of 1 because we want to output one value\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=96, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=96,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=96,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=96))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=1))\n",
        "\n",
        "# reshape data into 3d array for LSTM because it is sequential_3 which is expecting 3 dimensions, not 2\n",
        "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
        "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
        "\n",
        "# now compile the model\n",
        "# used loss='mean_squared_error' because it is a regression problem\n",
        "# use the adam optimizer to update network weights iteratively based on training data\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Save model and start training!\n",
        "# Every epoch refers to one cycle through the full training dataset\n",
        "# batch size refers to the number of training examples utilized in one iteration\n",
        "model.fit(x_train, y_train, epochs=50, batch_size=32)\n",
        "model.save('stock_prediction.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load fitted model\n",
        "model = load_model('stock_prediction.h5')\n",
        "\n",
        "# visualize data\n",
        "predictions = model.predict(x_test)\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "y_test_scaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16,8))\n",
        "ax.set_facecolor('#000041')\n",
        "ax.plot(y_test_scaled, color='red', label='Original price')\n",
        "plt.plot(predictions, color='cyan', label='Predicted price')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "maQpBnrEwmpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# flattens a list of lists into 1 list\n",
        "def flatten(l):\n",
        "    return [item for sublist in l for item in sublist]\n",
        "\n",
        "def mean_absolute_error(actual, predicted):\n",
        "  return abs(sum(flatten(actual)) - sum(flatten(predicted))) / predictions.size\n",
        "\n",
        "def mean_squared_error(actual, predicted):\n",
        "  return math.sqrt(abs(sum(flatten(y_test_scaled)) - sum(flatten(predictions)))**2 / predictions.size)\n",
        "\n",
        "def mean_absolute_percentage_error(actual, predicted):\n",
        "  error = sum(abs(actual - predicted) / abs(actual)) / predictions.size * 100\n",
        "  return error[0]\n",
        "\n",
        "# calculate Mean Absolute Error\n",
        "MAE = mean_absolute_error(y_test_scaled, predictions)\n",
        "print(f\"Mean Absolute Error: {MAE}\")\n",
        "\n",
        "# calculate Mean Squared Error\n",
        "MSE = mean_squared_error(y_test_scaled, predictions)\n",
        "print(f\"Mean Squared Error: {MSE}\")\n",
        "\n",
        "# calculate Mean Absolute Percentage Error\n",
        "MAPE = mean_absolute_percentage_error(y_test_scaled, predictions)\n",
        "print(f\"Mean Absolute Percentage Error: {MAPE}\")\n",
        "\n",
        "# calculate Mean Absolute Scaled Error\n",
        "# compares current model error with previous model error by a ratio (<1 means new model better, >1 means old model better)\n",
        "# add this code when the time comes to make models more precise\n",
        "MASE = MAE_current / MAE_previous\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xFSxX3xx0_M",
        "outputId": "9e43bc57-a468-4f43-ebb5-925d49274e5c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 16.447479323585434\n",
            "Mean Squared Error: 233.76260259535445\n",
            "Mean Absolute Percentage Error: 8.538594516407883\n"
          ]
        }
      ]
    }
  ]
}