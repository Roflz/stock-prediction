{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM+7kONuC7R+ur90nP1z6YC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Installs/upgrades packages needed for running stock prediction program using Google Colab\n",
        "# After installing/upgrading, Go to Runtime -> Restart runtime to apply changes\n",
        "\n",
        "!pip install --upgrade pandas-datareader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "WM_mXb8M_vm9",
        "outputId": "2ece64ea-8336-455a-ba10-d8b48846ca7c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas-datareader in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Collecting pandas-datareader\n",
            "  Downloading pandas_datareader-0.10.0-py3-none-any.whl (109 kB)\n",
            "\u001b[?25l\r\u001b[K     |███                             | 10 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 20 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 30 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 40 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 51 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 71 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 81 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 102 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 109 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from pandas-datareader) (1.3.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pandas-datareader) (4.9.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pandas-datareader) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->pandas-datareader) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->pandas-datareader) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->pandas-datareader) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23->pandas-datareader) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader) (2.10)\n",
            "Installing collected packages: pandas-datareader\n",
            "  Attempting uninstall: pandas-datareader\n",
            "    Found existing installation: pandas-datareader 0.9.0\n",
            "    Uninstalling pandas-datareader-0.9.0:\n",
            "      Successfully uninstalled pandas-datareader-0.9.0\n",
            "Successfully installed pandas-datareader-0.10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas_datareader"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "id": "WBAlO5uNlGAi",
        "outputId": "137cda20-509b-4aa2-e38e-29ab372b5f59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  High         Low        Open       Close       Volume  \\\n",
            "Date                                                                      \n",
            "2017-08-07   43.092499   42.000000   42.097500   43.087502   71741200.0   \n",
            "2017-08-08   43.639999   42.177502   43.472500   42.575001   75533600.0   \n",
            "2017-08-09   43.052502   41.917500   42.107498   43.027500   53078400.0   \n",
            "2017-08-10   43.165001   41.082500   43.040001   41.185001  132808800.0   \n",
            "2017-08-11   39.750000   38.227501   39.285000   38.990002  149719600.0   \n",
            "...                ...         ...         ...         ...          ...   \n",
            "2022-07-28  181.399994  174.399994  179.750000  179.839996   47464600.0   \n",
            "2022-07-29  182.440002  176.919998  178.130005  181.630005   43520200.0   \n",
            "2022-08-01  188.460007  179.899994  181.820007  184.410004   47646900.0   \n",
            "2022-08-02  189.380005  180.919998  181.220001  185.259995   48952700.0   \n",
            "2022-08-03  189.679993  181.369995  181.839996  188.929993   41773400.0   \n",
            "\n",
            "             Adj Close  \n",
            "Date                    \n",
            "2017-08-07   42.609215  \n",
            "2017-08-08   42.102406  \n",
            "2017-08-09   42.549885  \n",
            "2017-08-10   40.727840  \n",
            "2017-08-11   38.557201  \n",
            "...                ...  \n",
            "2022-07-28  179.839996  \n",
            "2022-07-29  181.630005  \n",
            "2022-08-01  184.410004  \n",
            "2022-08-02  185.259995  \n",
            "2022-08-03  188.929993  \n",
            "\n",
            "[1257 rows x 6 columns]\n",
            "Epoch 1/50\n",
            "30/30 [==============================] - 14s 243ms/step - loss: 0.0165\n",
            "Epoch 2/50\n",
            " 2/30 [=>............................] - ETA: 4s - loss: 0.0044"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-8952eada9196>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# Every epoch refers to one cycle through the full training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# batch size refers to the number of training examples utilized in one iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stock_prediction.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas_datareader as pdr\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from google.colab import files\n",
        "\n",
        "# function to help create the datasets\n",
        "# for features (x), appends the last 50 prices\n",
        "# for labels (y), appends the next price\n",
        "def create_dataset(df):\n",
        "    x = []\n",
        "    y = []\n",
        "    for i in range(50, df.shape[0]):\n",
        "        x.append(df[i-50:i, 0])\n",
        "        y.append(df[i, 0])\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    return x,y\n",
        "\n",
        "# flattens a list of lists into 1 list\n",
        "def flatten(l):\n",
        "    return [item for sublist in l for item in sublist]\n",
        "\n",
        "# Request data via Yahoo public API\n",
        "# currently reads in 5 years of data as default\n",
        "df = pdr.get_data_yahoo('NVDA')\n",
        "print(df)\n",
        "\n",
        "# upload files and read csv (optional)\n",
        "# df = files.upload()\n",
        "# df = pd.read_csv(df)\n",
        "\n",
        "# get number of trading days\n",
        "df.shape\n",
        "\n",
        "# set dimensions of dataset\n",
        "df = df['Open'].values\n",
        "df = df.reshape(-1, 1)\n",
        "\n",
        "# split the data into training and testing sets\n",
        "# training set is taking 1st 20% of data points (Oldest 20% of data points)\n",
        "# test set is taking last 80% of data points (Most recent 80% of data points)\n",
        "dataset_train = np.array(df[:int(df.shape[0]*0.8)])\n",
        "dataset_test = np.array(df[int(df.shape[0]*0.8):])\n",
        "\n",
        "# scale data between 0 and 1\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "dataset_train = scaler.fit_transform(dataset_train)\n",
        "dataset_test = scaler.transform(dataset_test)\n",
        "\n",
        "# create datasets using function\n",
        "x_train, y_train = create_dataset(dataset_train)\n",
        "x_test, y_test = create_dataset(dataset_test)\n",
        "\n",
        "# initialize model as a sequential one with 96 units in the output’s dimensionality\n",
        "# use return_sequences=True to make the LSTM layer with three-dimensional input and input_shape to shape our dataset\n",
        "# Making the dropout fraction 0.2 drops 20% of the layers\n",
        "# Finally add a dense layer with a value of 1 because we want to output one value\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=96, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=96,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=96,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=96))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=1))\n",
        "\n",
        "# reshape data into 3d array for LSTM because it is sequential_3 which is expecting 3 dimensions, not 2\n",
        "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
        "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
        "\n",
        "# now compile the model\n",
        "# used loss='mean_squared_error' because it is a regression problem\n",
        "# use the adam optimizer to update network weights iteratively based on training data\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Save model and start training!\n",
        "# Every epoch refers to one cycle through the full training dataset\n",
        "# batch size refers to the number of training examples utilized in one iteration\n",
        "model.fit(x_train, y_train, epochs=50, batch_size=32)\n",
        "model.save('stock_prediction.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load fitted model\n",
        "model = load_model('stock_prediction.h5')\n",
        "\n",
        "# visualize data\n",
        "predictions = model.predict(x_test)\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "y_test_scaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16,8))\n",
        "ax.set_facecolor('#000041')\n",
        "ax.plot(y_test_scaled, color='red', label='Original price')\n",
        "plt.plot(predictions, color='cyan', label='Predicted price')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "maQpBnrEwmpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "# flattens a list of lists into 1 list\n",
        "def flatten(l):\n",
        "    return [item for sublist in l for item in sublist]\n",
        "\n",
        "def mean_absolute_error(actual, predicted):\n",
        "  return abs(sum(flatten(actual)) - sum(flatten(predicted))) / predictions.size\n",
        "\n",
        "def mean_squared_error(actual, predicted):\n",
        "  return math.sqrt(abs(sum(flatten(y_test_scaled)) - sum(flatten(predictions)))**2 / predictions.size)\n",
        "\n",
        "def mean_absolute_percentage_error(actual, predicted):\n",
        "  return sum(abs(actual - predicted) / abs(actual) / predictions.size * 100\n",
        "\n",
        "# calculate Mean Absolute Error\n",
        "MAE = mean_absolute_error(y_test_scaled, predictions)\n",
        "print(f\"Mean Absolute Error: {MAE}\")\n",
        "\n",
        "# calculate Mean Squared Error\n",
        "MSE = mean_squared_error(y_test_scaled, predictions)\n",
        "print(f\"Mean Squared Error: {MSE}\")\n",
        "\n",
        "# calculate Mean Absolute Percentage Error\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xFSxX3xx0_M",
        "outputId": "69f3edf9-f89b-45db-9161-1fb5c78c23aa"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 16.447479323585434\n",
            "Mean Squared Error: 233.76260259535445\n"
          ]
        }
      ]
    }
  ]
}